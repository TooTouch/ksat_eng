{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "determined-flour",
   "metadata": {},
   "source": [
    "# 주제 / 제목 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-authentication",
   "metadata": {},
   "source": [
    "### 참고자료\n",
    "* Sentence-Transformer: https://huggingface.co/sentence-transformers/bert-base-nli-mean-tokens\n",
    "* TextRank: yukyunglee Github > https://github.com/yukyunglee/TextRank\n",
    "* Document Similarity: https://towardsdatascience.com/calculating-document-similarities-using-bert-and-other-models-b2c1a29c9630\n",
    "* BERT Word Embedding: https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "* BERT Code Visual Guide: https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "several-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import BertForNextSentencePrediction, BertTokenizer, BertModel, AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "distributed-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "plain-halifax",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_bert_wordemb(sentence):\n",
    "    marked_sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_sentence)\n",
    "    \n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    \n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        hidden_states = outputs[2]\n",
    "        \n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    token_embeddings.size()\n",
    "\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    \n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "        \n",
    "    return token_vecs_sum\n",
    "\n",
    "    # print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n",
    "    \n",
    "    \n",
    "# Sentence Format: 'i need calcium and something else for replenish myself'\n",
    "def sentence_to_bert_sentemb(sentence):\n",
    "    marked_sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_sentence)\n",
    "    \n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    \n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # `token_vecs` is a tensor with shape [22 x 768]\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "\n",
    "    # Calculate the average of all 22 token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    \n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "accepted-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = sentence_to_bert_wordemb('i need calcium and something else to do')\n",
    "sentence_embddding = sentence_to_bert_sentemb('i need calcium and something else to do')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "restricted-brand",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-ad2d48547a57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_text' is not defined"
     ]
    }
   ],
   "source": [
    "sentences = test_text.split('.')\n",
    "sentences = [s.strip()+'.' for s in sentences][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-assets",
   "metadata": {},
   "source": [
    "### Sentence Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-russia",
   "metadata": {},
   "source": [
    "### 다음 글의 제목으로 가장 적절한 것은?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "refined-radius",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People don’t usually think of touch as a temporal phenomenon, but it is every bit as time-based as it is spatial. You can carry out an experiment to see for yourself. Ask a friend to cup his hand, palm face up, and close his eyes. Place a small ordinary object in his palm ― a ring, an eraser, anything will do ― and ask him to identify it without moving any part of his hand. He won’t have a clue other than weight and maybe overall size. Then tell him to keep his eyes closed and move his fingers over the object. He’ll most likely identify it at once. By allowing the fingers to move, you’ve added time to the sensory perception of touch. There’s a direct analogy between the fovea at the center of your retina and your fingertips, both of which have high acuity. Your ability to make complex use of touch, such as buttoning your shirt or unlocking your front door in the dark, depends on continuous time-varying patterns of touch sensation.\n"
     ]
    }
   ],
   "source": [
    "# Answer: 3\n",
    "test_text = \"People don’t usually think of touch as a temporal phenomenon, but it is every bit as time-based as it is spatial. You can carry out an experiment to see for yourself. Ask a friend to cup his hand, palm face up, and close his eyes. Place a small ordinary object in his palm ― a ring, an eraser, anything will do ― and ask him to identify it without moving any part of his hand. He won’t have a clue other than weight and maybe overall size. Then tell him to keep his eyes closed and move his fingers over the object. He’ll most likely identify it at once. By allowing the fingers to move, you’ve added time to the sensory perception of touch. There’s a direct analogy between the fovea at the center of your retina and your fingertips, both of which have high acuity. Your ability to make complex use of touch, such as buttoning your shirt or unlocking your front door in the dark, depends on continuous time-varying patterns of touch sensation.\"\n",
    "print(test_text)\n",
    "\n",
    "one = sentence_to_bert_sentemb('Touch and Movement: Two Major Elements of Humanity')\n",
    "two = sentence_to_bert_sentemb('The Role of Touch in Forming the Concept of Time')\n",
    "thr = sentence_to_bert_sentemb('Time Does Matter: A Hidden Essence of Touch')\n",
    "fou = sentence_to_bert_sentemb('How to Use the Five Senses in a Timely Manner')\n",
    "fiv = sentence_to_bert_sentemb('The Surprising Function of Touch as a Booster of Knowledge')\n",
    "\n",
    "answers = [one, two, thr, fou, fiv]\n",
    "# for idx, answer in enumerate(answers):\n",
    "#     print(f'({idx}): {answer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-assets",
   "metadata": {},
   "source": [
    "### 다음 글의 주제로 가장 적절한 것은?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "cutting-flesh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difficulties arise when we do not think of people and machines as collaborative systems, but assign whatever tasks can be automated to the machines and leave the rest to people. This ends up requiring people to behave in machine-like fashion, in ways that differ from human capabilities. We expect people to monitor machines, which means keeping alert for long periods, something we are bad at. We require people to do repeated operations with the extreme precision and accuracy required by machines, again something we are not good at. When we divide up the machine and human components of a task in this way, we fail to take advantage of human strengths and capabilities but instead rely upon areas where we are genetically, biologically unsuited. Yet, when people fail, they are blamed.\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Difficulties arise when we do not think of people and machines as collaborative systems, but assign whatever tasks can be automated to the machines and leave the rest to people. This ends up requiring people to behave in machine-like fashion, in ways that differ from human capabilities. We expect people to monitor machines, which means keeping alert for long periods, something we are bad at. We require people to do repeated operations with the extreme precision and accuracy required by machines, again something we are not good at. When we divide up the machine and human components of a task in this way, we fail to take advantage of human strengths and capabilities but instead rely upon areas where we are genetically, biologically unsuited. Yet, when people fail, they are blamed.\"\n",
    "print(test_text)\n",
    "\n",
    "one = sentence_to_bert_sentemb('difficulties of overcoming human weaknesses to avoid failure')\n",
    "two = sentence_to_bert_sentemb('issues of allocating unfit tasks to humans in automated systems')\n",
    "thr = sentence_to_bert_sentemb('benefits of allowing machines and humans to work together')\n",
    "fou = sentence_to_bert_sentemb('reasons why humans continue to pursue machine automation')\n",
    "fiv = sentence_to_bert_sentemb('influences of human actions on a machine’s performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "cordless-manitoba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human beings do not enter the world as competent moral agents. Nor does everyone leave the world in that state. But somewhere in between, most people acquire a bit of decency that qualifies them for membership in the community of moral agents. Genes, development, and learning all contribute to the process of becoming a decent human being. The interaction between nature and nurture is, however, highly complex, and developmental biologists are only just beginning to grasp just how complex it is. Without the context provided by cells, organisms, social groups, and culture, DNA is inert. Anyone who says that people are “genetically programmed” to be moral has an oversimplified view of how genes work. Genes and environment interact in ways that make it nonsensical to think that the process of moral development in children, or any other developmental process, can be discussed in terms of nature versus nurture. Developmental biologists now know that it is really both, or nature through nurture. A complete scientific explanation of moral evolution and development in the human species is a very long way off.\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Human beings do not enter the world as competent moral agents. Nor does everyone leave the world in that state. But somewhere in between, most people acquire a bit of decency that qualifies them for membership in the community of moral agents. Genes, development, and learning all contribute to the process of becoming a decent human being. The interaction between nature and nurture is, however, highly complex, and developmental biologists are only just beginning to grasp just how complex it is. Without the context provided by cells, organisms, social groups, and culture, DNA is inert. Anyone who says that people are “genetically programmed” to be moral has an oversimplified view of how genes work. Genes and environment interact in ways that make it nonsensical to think that the process of moral development in children, or any other developmental process, can be discussed in terms of nature versus nurture. Developmental biologists now know that it is really both, or nature through nurture. A complete scientific explanation of moral evolution and development in the human species is a very long way off.\"\n",
    "print(test_text)\n",
    "\n",
    "one = sentence_to_bert_sentemb('evolution of human morality from a cultural perspective ')\n",
    "two = sentence_to_bert_sentemb('difficulties in studying the evolutionary process of genes')\n",
    "thr = sentence_to_bert_sentemb('increasing necessity of educating children as moral agents')\n",
    "fou = sentence_to_bert_sentemb('nature versus nurture controversies in developmental biology')\n",
    "fiv = sentence_to_bert_sentemb('complicated gene-environment interplay in moral development ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "limited-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "# 코사인 유사도 함수 \n",
    "def cos_sim(A, B):\n",
    "    return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "def return_max_similarity(summary, answers):\n",
    "    maxi = 0\n",
    "    for i, sent_emb in enumerate(answers):\n",
    "        similarity = cos_sim(summary, sent_emb)\n",
    "        print(f'answer {i}\\'s similarity with summary is {similarity}')\n",
    "        \n",
    "        if maxi < similarity:\n",
    "            maxi = similarity\n",
    "\n",
    "    return maxi\n",
    "\n",
    "# return_max(summary, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "driving-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "import networkx as nx\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "sent_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/bert-base-nli-cls-token\")\n",
    "sent_model = AutoModel.from_pretrained(\"sentence-transformers/bert-base-nli-cls-token\")    \n",
    "\n",
    "def sentence_to_bert_emb(sentence):\n",
    "    marked_sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(add_special_token(marked_sentence))\n",
    "    \n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    \n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # `token_vecs` is a tensor with shape [22 x 768]\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "\n",
    "    # Calculate the average of all 22 token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    \n",
    "    return sentence_embedding\n",
    "\n",
    "\n",
    "def sentences_to_bert_sentemb(sentences):\n",
    "    encoded_input = sent_tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = sent_model(**encoded_input)\n",
    "        sentence_embeddings = model_output[0][:,0] #Take the first token ([CLS]) from each sentence \n",
    "\n",
    "    return sentence_embeddings\n",
    "\n",
    "def make_sentence_graph(sentence, min_sim):\n",
    "    sentence_graph = nx.Graph()  # initialize an undirected graph\n",
    "    sentence_graph.add_nodes_from(sentence)\n",
    "\n",
    "    nodePairs = list(itertools.combinations(sentence, 2))\n",
    "\n",
    "    # add edges to the graph (weighted by Levenshtein distance)\n",
    "    for pair in nodePairs:\n",
    "        node1 = pair[0]\n",
    "        node2 = pair[1]\n",
    "\n",
    "        cos_sim = dot(sentence[pair[0]][1], sentence[pair[1]][1]) / (\n",
    "            norm(sentence[pair[0]][1]) * norm(sentence[pair[1]][1])\n",
    "        )\n",
    "\n",
    "        if cos_sim > min_sim:\n",
    "            sentence_graph.add_edge(node1, node2, weight=cos_sim)\n",
    "\n",
    "    return sentence_graph\n",
    "\n",
    "\n",
    "def extract_sentence(sentence_graph, sentence, top_k):\n",
    "    calculated_page_rank = nx.pagerank(\n",
    "        sentence_graph, alpha=0.85, max_iter=100, weight=\"weight\"\n",
    "    )\n",
    "\n",
    "    sentences = sorted(calculated_page_rank, key=calculated_page_rank.get, reverse=True)\n",
    "\n",
    "    modified_sentence = sentences[: -len(sentences) + top_k]\n",
    "    result_sentence = [(sentence[sent][0], sent) for sent in modified_sentence]\n",
    "\n",
    "    return result_sentence\n",
    "\n",
    "def sentence_summary(article, min_sim, top_k, cleaned = True):\n",
    "\n",
    "    sentence_sum_result = []\n",
    "\n",
    "    original_sentences = [s for s in article.split('.')]\n",
    "\n",
    "    # sentence마다의 embedding이 포함되어 있다.\n",
    "    article_embs = [sentence_to_bert_sentemb(s) for s in original_sentences]\n",
    "    sentence = {}\n",
    "\n",
    "    for idx, sent in enumerate(article_embs):\n",
    "        sentence[original_sentences[idx]] = [idx, sent]\n",
    "\n",
    "    sentence_graph = make_sentence_graph(sentence, min_sim=min_sim)\n",
    "\n",
    "    extracted_sentence = extract_sentence(sentence_graph, sentence, top_k=top_k)\n",
    "    sentence_sum_result.append(extracted_sentence)\n",
    "    \n",
    "    if cleaned:\n",
    "        sentence_sum_result = [i[1] for i in sentence_sum_result[0]]\n",
    "        sentence_sum_result = '.'.join(sentence_sum_result)\n",
    "\n",
    "    return sentence_sum_result\n",
    "\n",
    "# need the whold article / not a single sentence\n",
    "def sentence_summary_sentemb(article, min_sim, top_k, cleaned=True):\n",
    "    sentence_sum_result = []\n",
    "    original_sentences = [i.strip() for i in article.split('.')][:-1]\n",
    "    \n",
    "    # sentence마다의 embedding이 포함되어 있다.\n",
    "    article_embs = sentences_to_bert_sentemb(original_sentences)\n",
    "    sentence = {}\n",
    "\n",
    "    for idx, sent in enumerate(article_embs):\n",
    "        sentence[original_sentences[idx]] = [idx, sent]\n",
    "\n",
    "    sentence_graph = make_sentence_graph(sentence, min_sim=min_sim)\n",
    "\n",
    "    extracted_sentence = extract_sentence(sentence_graph, sentence, top_k=top_k)\n",
    "    sentence_sum_result.append(extracted_sentence)\n",
    "    \n",
    "    if cleaned:\n",
    "        sentence_sum_result = [i[1] for i in sentence_sum_result[0]]\n",
    "        sentence_sum_result = '.'.join(sentence_sum_result)\n",
    "\n",
    "    return sentence_sum_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "tamil-house",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People don’t usually think of touch as a temporal phenomenon, but it is every bit as time-based as it is spatial. You can carry out an experiment to see for yourself. Ask a friend to cup his hand, palm face up, and close his eyes. Place a small ordinary object in his palm ― a ring, an eraser, anything will do ― and ask him to identify it without moving any part of his hand. He won’t have a clue other than weight and maybe overall size. Then tell him to keep his eyes closed and move his fingers over the object. He’ll most likely identify it at once. By allowing the fingers to move, you’ve added time to the sensory perception of touch. There’s a direct analogy between the fovea at the center of your retina and your fingertips, both of which have high acuity. Your ability to make complex use of touch, such as buttoning your shirt or unlocking your front door in the dark, depends on continuous time-varying patterns of touch sensation.\n"
     ]
    }
   ],
   "source": [
    "# Answer: 3\n",
    "test_text = \"People don’t usually think of touch as a temporal phenomenon, but it is every bit as time-based as it is spatial. You can carry out an experiment to see for yourself. Ask a friend to cup his hand, palm face up, and close his eyes. Place a small ordinary object in his palm ― a ring, an eraser, anything will do ― and ask him to identify it without moving any part of his hand. He won’t have a clue other than weight and maybe overall size. Then tell him to keep his eyes closed and move his fingers over the object. He’ll most likely identify it at once. By allowing the fingers to move, you’ve added time to the sensory perception of touch. There’s a direct analogy between the fovea at the center of your retina and your fingertips, both of which have high acuity. Your ability to make complex use of touch, such as buttoning your shirt or unlocking your front door in the dark, depends on continuous time-varying patterns of touch sensation.\"\n",
    "print(test_text)\n",
    "\n",
    "one = sentence_to_bert_sentemb('Touch and Movement: Two Major Elements of Humanity')\n",
    "two = sentence_to_bert_sentemb('The Role of Touch in Forming the Concept of Time')\n",
    "thr = sentence_to_bert_sentemb('Time Does Matter: A Hidden Essence of Touch')\n",
    "fou = sentence_to_bert_sentemb('How to Use the Five Senses in a Timely Manner')\n",
    "fiv = sentence_to_bert_sentemb('The Surprising Function of Touch as a Booster of Knowledge')\n",
    "\n",
    "answers = [one, two, thr, fou, fiv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "classified-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = sentence_summary(test_text, min_sim=0.9, top_k=3, cleaned = True)\n",
    "sentemb_summary = sentence_summary_sentemb(test_text, min_sim=0.9, top_k=3, cleaned = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "naughty-athletics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People don’t usually think of touch as a temporal phenomenon, but it is every bit as time-based as it is spatial. You can carry out an experiment to see for yourself. Ask a friend to cup his hand, palm face up, and close his eyes\n",
      "People don’t usually think of touch as a temporal phenomenon, but it is every bit as time-based as it is spatial.You can carry out an experiment to see for yourself.Ask a friend to cup his hand, palm face up, and close his eyes\n"
     ]
    }
   ],
   "source": [
    "print(summary)\n",
    "print(sentemb_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "universal-velvet",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = sentence_to_bert_sentemb(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "outer-ecuador",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer 0's similarity with summary is 0.7587635517120361\n",
      "answer 1's similarity with summary is 0.7747366428375244\n",
      "answer 2's similarity with summary is 0.7757629156112671\n",
      "answer 3's similarity with summary is 0.7234643697738647\n",
      "answer 4's similarity with summary is 0.7858073115348816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7858073"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_max_similarity(summary, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-ceiling",
   "metadata": {},
   "source": [
    "# 실제 문제 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "downtown-theater",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People don’t usually think of touch as a temporal phenomenon, but it is every bit as time-based as it is spatial. You can carry out an experiment to see for yourself. Ask a friend to cup his hand, palm face up, and close his eyes. Place a small ordinary object in his palm ― a ring, an eraser, anything will do ― and ask him to identify it without moving any part of his hand. He won’t have a clue other than weight and maybe overall size. Then tell him to keep his eyes closed and move his fingers over the object. He’ll most likely identify it at once. By allowing the fingers to move, you’ve added time to the sensory perception of touch. There’s a direct analogy between the fovea at the center of your retina and your fingertips, both of which have high acuity. Your ability to make complex use of touch, such as buttoning your shirt or unlocking your front door in the dark, depends on continuous time-varying patterns of touch sensation.\n"
     ]
    }
   ],
   "source": [
    "# Answer: 3\n",
    "test_text = \"People don’t usually think of touch as a temporal phenomenon, but it is every bit as time-based as it is spatial. You can carry out an experiment to see for yourself. Ask a friend to cup his hand, palm face up, and close his eyes. Place a small ordinary object in his palm ― a ring, an eraser, anything will do ― and ask him to identify it without moving any part of his hand. He won’t have a clue other than weight and maybe overall size. Then tell him to keep his eyes closed and move his fingers over the object. He’ll most likely identify it at once. By allowing the fingers to move, you’ve added time to the sensory perception of touch. There’s a direct analogy between the fovea at the center of your retina and your fingertips, both of which have high acuity. Your ability to make complex use of touch, such as buttoning your shirt or unlocking your front door in the dark, depends on continuous time-varying patterns of touch sensation.\"\n",
    "print(test_text)\n",
    "\n",
    "one = sentence_to_bert_sentemb('Touch and Movement: Two Major Elements of Humanity')\n",
    "two = sentence_to_bert_sentemb('The Role of Touch in Forming the Concept of Time')\n",
    "thr = sentence_to_bert_sentemb('Time Does Matter: A Hidden Essence of Touch')\n",
    "fou = sentence_to_bert_sentemb('How to Use the Five Senses in a Timely Manner')\n",
    "fiv = sentence_to_bert_sentemb('The Surprising Function of Touch as a Booster of Knowledge')\n",
    "\n",
    "answers = [one, two, thr, fou, fiv]\n",
    "# for idx, answer in enumerate(answers):\n",
    "#     print(f'({idx}): {answer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "introductory-oriental",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difficulties arise when we do not think of people and machines as collaborative systems, but assign whatever tasks can be automated to the machines and leave the rest to people. This ends up requiring people to behave in machine-like fashion, in ways that differ from human capabilities. We expect people to monitor machines, which means keeping alert for long periods, something we are bad at. We require people to do repeated operations with the extreme precision and accuracy required by machines, again something we are not good at. When we divide up the machine and human components of a task in this way, we fail to take advantage of human strengths and capabilities but instead rely upon areas where we are genetically, biologically unsuited. Yet, when people fail, they are blamed.\n"
     ]
    }
   ],
   "source": [
    "# 2021 영어 영역 짝수형 23\n",
    "# 정답 2\n",
    "\n",
    "test_text = \"Difficulties arise when we do not think of people and machines as collaborative systems, but assign whatever tasks can be automated to the machines and leave the rest to people. This ends up requiring people to behave in machine-like fashion, in ways that differ from human capabilities. We expect people to monitor machines, which means keeping alert for long periods, something we are bad at. We require people to do repeated operations with the extreme precision and accuracy required by machines, again something we are not good at. When we divide up the machine and human components of a task in this way, we fail to take advantage of human strengths and capabilities but instead rely upon areas where we are genetically, biologically unsuited. Yet, when people fail, they are blamed.\"\n",
    "print(test_text)\n",
    "\n",
    "one = sentence_to_bert_sentemb('difficulties of overcoming human weaknesses to avoid failure')\n",
    "two = sentence_to_bert_sentemb('issues of allocating unfit tasks to humans in automated systems')\n",
    "thr = sentence_to_bert_sentemb('benefits of allowing machines and humans to work together')\n",
    "fou = sentence_to_bert_sentemb('reasons why humans continue to pursue machine automation')\n",
    "fiv = sentence_to_bert_sentemb('influences of human actions on a machine’s performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "thirty-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human beings do not enter the world as competent moral agents. Nor does everyone leave the world in that state. But somewhere in between, most people acquire a bit of decency that qualifies them for membership in the community of moral agents. Genes, development, and learning all contribute to the process of becoming a decent human being. The interaction between nature and nurture is, however, highly complex, and developmental biologists are only just beginning to grasp just how complex it is. Without the context provided by cells, organisms, social groups, and culture, DNA is inert. Anyone who says that people are “genetically programmed” to be moral has an oversimplified view of how genes work. Genes and environment interact in ways that make it nonsensical to think that the process of moral development in children, or any other developmental process, can be discussed in terms of nature versus nurture. Developmental biologists now know that it is really both, or nature through nurture. A complete scientific explanation of moral evolution and development in the human species is a very long way off.\n"
     ]
    }
   ],
   "source": [
    "# 2020 영어 영역 홀수형 23\n",
    "# 정답 5\n",
    "\n",
    "test_text = \"Human beings do not enter the world as competent moral agents. Nor does everyone leave the world in that state. But somewhere in between, most people acquire a bit of decency that qualifies them for membership in the community of moral agents. Genes, development, and learning all contribute to the process of becoming a decent human being. The interaction between nature and nurture is, however, highly complex, and developmental biologists are only just beginning to grasp just how complex it is. Without the context provided by cells, organisms, social groups, and culture, DNA is inert. Anyone who says that people are “genetically programmed” to be moral has an oversimplified view of how genes work. Genes and environment interact in ways that make it nonsensical to think that the process of moral development in children, or any other developmental process, can be discussed in terms of nature versus nurture. Developmental biologists now know that it is really both, or nature through nurture. A complete scientific explanation of moral evolution and development in the human species is a very long way off.\"\n",
    "print(test_text)\n",
    "\n",
    "one = sentence_to_bert_sentemb('evolution of human morality from a cultural perspective ')\n",
    "two = sentence_to_bert_sentemb('difficulties in studying the evolutionary process of genes')\n",
    "thr = sentence_to_bert_sentemb('increasing necessity of educating children as moral agents')\n",
    "fou = sentence_to_bert_sentemb('nature versus nurture controversies in developmental biology')\n",
    "fiv = sentence_to_bert_sentemb('complicated gene-environment interplay in moral development ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "immediate-company",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difficulties arise when we do not think of people and machines as collaborative systems, but assign whatever tasks can be automated to the machines and leave the rest to people. When we divide up the machine and human components of a task in this way, we fail to take advantage of human strengths and capabilities but instead rely upon areas where we are genetically, biologically unsuited. This ends up requiring people to behave in machine-like fashion, in ways that differ from human capabilities\n",
      "Difficulties arise when we do not think of people and machines as collaborative systems, but assign whatever tasks can be automated to the machines and leave the rest to people.This ends up requiring people to behave in machine-like fashion, in ways that differ from human capabilities.We expect people to monitor machines, which means keeping alert for long periods, something we are bad at\n",
      "answer 0's similarity with summary is 0.7676581740379333\n",
      "answer 1's similarity with summary is 0.7434587478637695\n",
      "answer 2's similarity with summary is 0.7229117751121521\n",
      "answer 3's similarity with summary is 0.7313324213027954\n",
      "answer 4's similarity with summary is 0.7358259558677673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7676582"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = sentence_summary(test_text, min_sim=0.9, top_k=3, cleaned = True)\n",
    "sentemb_summary = sentence_summary_sentemb(test_text, min_sim=0.9, top_k=3, cleaned = True)\n",
    "\n",
    "print(summary)\n",
    "print(sentemb_summary)\n",
    "\n",
    "summary = sentence_to_bert_sentemb(summary)\n",
    "return_max_similarity(summary, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-soldier",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
